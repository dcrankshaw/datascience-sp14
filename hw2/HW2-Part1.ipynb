{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CS194-16 Introduction to Data Science\n",
      "\n",
      "**Name**: *Please put your name*\n",
      "\n",
      "**Student ID**: *Please put your student ID*\n",
      "\n",
      "\n",
      "Assignment 2: Introduction to Machine Learning: Clustering and Classification\n",
      "===\n",
      "\n",
      "## Overview\n",
      "\n",
      "In this assignment, we will use machine learning techniques to perform data analysis and learn models about our data. We will use a real world music dataset from [Last.fm](http://last.fm) for this assignment. There are two parts to this assignment: In the first part we will look at Unsupervised Learning with clustering and in the second part, we will study Supervised Learning."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction to Machine Learning\n",
      "\n",
      "Machine learning is a branch of artifical intelligence where we try to find hidden structure within data. \n",
      "For example, lets say you are hired as a data scientist at a cool new music playing startup. You are given access to \n",
      "logs from the product and are asked find out what kinds of music are played on your website and how you can promote songs that will be \n",
      "popular. In this case we wish to extract some structure from the raw data we have using machine learning.\n",
      "\n",
      "There are two main kinds of machine learning algorithms:\n",
      "Unsupervised learning is the branch where we don't have any ground truth (or labeled data) that can help\n",
      "our training process. There are many approaches to unsupervised learning which includes topics like Clustering, \n",
      "Mixture Models, Hidden Markov Models etc. In this assignment we will predominantly look at clustering.\n",
      "In supervised learning we have training data which is labeled (either manually or from historical data) and we try to make\n",
      "predictions on new data.  **TODO: Same a few more words here**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Application\n",
      "\n",
      "Your assignment is to use machine learnings algorithms for two tasks on a real world music dataset from Last.fm. The goal in the first part is to cluster artists and try to discover all artists that belong a certain genre. **TODO: Say something about second part here**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Files\n",
      "\n",
      "Data files for this assignment can be found at:\n",
      "\n",
      "`https://github.com/amplab/datascience-sp14/raw/master/hw2/hw2data.tar.gz`\n",
      "\n",
      "The zip file includes the following files:\n",
      "\n",
      "* **artists-tags.csv**, User-defined tags for top artists\n",
      "* **userart-mat.csv**, Matrix mapping artist-id to users who have played songs by the artists\n",
      "\n",
      "We will explain the datasets and how they need to used in the assignment sections."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Deliverables\n",
      "\n",
      "Complete the all the exercises below and turn in a write up in the form of an IPython notebook, that is, **an .ipynb file**.\n",
      "The write up should include your code, answers to exercise questions, and plots of results.\n",
      "Complete submission instructions will be posted on Piazza.\n",
      "\n",
      "You can use this notebook and fill in answers inline, or if you prefer, do your write up in a separate notebook.\n",
      "In this notebook, we provide code templates for many of the exercises.\n",
      "They are intended to help with code re-use, since the exercises build on each other, and are highly recommended.\n",
      "Don't forget to include answers to questions that ask for natural language responses, i.e., in English, not code!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Guidelines\n",
      "\n",
      "#### Code\n",
      "\n",
      "This assignment can be done with basic python, matplotlib and scikit-learn.\n",
      "Feel free to use PANDAs, too, which you may find well suited to several exercises.\n",
      "As for other libraries, please check with course staff whether they're allowed.\n",
      "In general, we want you to use whatever is comfortable, except for libraries (e.g., NLTK) that include functionality covered in the assignment.\n",
      "\n",
      "You're not required to do your coding in IPython, so feel free to use your favorite editor or IDE.\n",
      "But when you're done, remember to put your code into a notebook for your write up.\n",
      "\n",
      "#### Collaboration\n",
      "\n",
      "This assignment is to be done individually.  Everyone should be getting a hands on experience in this course.  You are free to discuss course material with fellow students, and we encourage you to use Internet resources to aid your understanding, but the work you turn in, including all code and answers, must be your own work."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 0: Preliminaries\n",
      "\n",
      "### Exercise 0\n",
      "\n",
      "Download the data and unzip it. \n",
      "\n",
      "Read in the file **artists-tags.csv** and store the contents in a DataFrame or a list. The file format for this file is `artist-id|artists-name|tag`.\n",
      "\n",
      "Similarly, read in the file **userart-mat.csv** . The file format for this file is `artist-id, user1, user2, .... user1000`. i.e. There are 1000 columns in this file and each column has a value 1 if the particular user played a song from this artist."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATA_PATH = \"\" # Make this the /path/to/the/data\n",
      "artists_tags = None # Fill this with appropriate data\n",
      "user_art_mat = None # Fill this with appropriate data\n",
      "\n",
      "# TODO Load data files here..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 1: Finding genres by clustering\n",
      "\n",
      "The first task we will look at is how to discover artist genres by only looking at data from plays on Last.fm. One of the ways to do this is to use clustering. To evaluate how well our clustering algorithm performs we will use the user-generated tags and compare those to our clustering results. \n",
      "\n",
      "### 1.1 Data pre-processing\n",
      "\n",
      "**TODO**: Say something here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 1\n",
      "\n",
      "**a**. As a first step in this process, pre-process the raw data we have to determine the most appropriate tag for an artist. To do this use the data we read in and for every artist in **artists_tags** calculate the most frequently used tag. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Implement this. You can change the function arguments if necessary\n",
      "def calculate_top_tag():\n",
      "    pass\n",
      "\n",
      "# Print the top tag for Nirvana\n",
      "print \"Top tag for Nirvana is\" # Complete this line -- Artist ID for Nirvana is 003b2747-b74a-46c1-a51e-aeaffe88256c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Create a matrix from **user_art_mat**. To do clustering we will be using `numpy` matrices. Convert the values in 1000 with user plays into a matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_np_matrix = None # Fill this in\n",
      "\n",
      "print user_np_matrix.shape # Should be (22139, 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.2 K-Means clustering\n",
      "\n",
      "Having pre-processed the data we can now perform clustering on the dataset. In this assignment we will be using the python library \n",
      "[scikit-learn](http://scikit-learn.org/stable/index.html) for our machine learning algorithms. scikit-learn provides an extensive\n",
      "library of machine learning algorithms that can be used for analysis. Here is a [nice flow chart](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) that shows various algorithms implemented\n",
      "and when to use any of them. In this assignment we will look at K-Means clustering\n",
      "\n",
      "> **Note on terminology**: \"samples\" and \"features\" are two words you will come across frequently when you look at machine learning papers or documentation. \"samples\" refer to data points that are used as inputs to the machine learning algorithm. For example in our dataset each artist is a \"sample\". \"features\" refers to some representation we have for every sample. For example the list of 1s and 0s we have for each artist are \"features\". Similarly the bag-of-words approach from the previous homework produced \"features\" for each document.\n",
      "\n",
      "#### K-Means algorithm\n",
      "\n",
      "Clustering is the process of automatically grouping data points that are similar to each other. In the [K-Means algorithm](http://en.wikipedia.org/wiki/K-means_clustering) we start with `K` initially chosen cluster centers. We then assign every point to the closest center, computing the distance from the features. Next we update the cluster centers by averaging all the points in the cluster. Finally we repeat the algorithm until the cluster centers are stable.\n",
      "\n",
      "#### K-Means parameters\n",
      "From the above description we can see that there are a few parameters which control the K-Means algorithm. We will look at two parameters in the following exercises: \n",
      "\n",
      "1. Number of clusters: This needs to be chosen based on insights we have about the data. As we do not know how many genres exist we will try different values\n",
      "2. Initialization strategy: The basic initialization strategy for k-means is to pick random cluster centers initially. More recently a smart approach called 'kmeans++' has been proposed to improve convergence time. We wil compare both approaches"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 2\n",
      "\n",
      "**a**. Run K-means using *5* cluster centers on the `user_np_matrix` using *random* initialization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run K-means using 5 cluster centers on user_np_matrix using the random initialization\n",
      "kmeans_5_random = None\n",
      "\n",
      "print \"Time taken for clustering with 5 centers and random init \""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Run K-means using *5* cluster centers on the `user_np_matrix` using *kmeans++* initialization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run K-means using 5 cluster centers on user_np_matrix using the kmeans++ initialization\n",
      "kmeans_5_smart = None\n",
      "\n",
      "print \"Time taken for clustering with 5 centers and smart init \""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**. Run K-means using *25* cluster centers on the `user_np_matrix` using both *kmeans++* and random initialization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans_25_random = None\n",
      "kmeans_25_smart = None\n",
      "\n",
      "print \"Time taken for clustering with 25 centers with random init \"\n",
      "print \"Time taken for clustering with 25 centers with smart init \""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.3 Evaluating K-Means\n",
      "\n",
      "In addition to the performance comparisons we also wish to compare how good our clusters are. To do this we are first going to look at internal evaluation metrics. For internal evaluation we only use the input data and the clusters created and try to measure the quality of clusters created. We are going to use three metrics for this:\n",
      "\n",
      "#### Inertia\n",
      "Inertia is a metric that is used to estimate how close the data points in a cluster are. This is calculated as the sum of squared distance for each point to it's closest centroid, i.e., its assigned cluster center. The intution behind inertia is that clusters with lower inertia are better as it means closely related points form a cluster.Inertia is calculated by scikit-learn by default.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print inertia for all the kmeans model computed above. Which is a better model ? Also explain why ?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Silhouette Score: \n",
      "The silhouette score measures how close various clusters created are. A higher silhouette score is better as it means that we dont have too many overlapping clusters. The silhouette score can be computed using `sklearn.metrics.silhouette_score` from scikit learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate the Silhouette Score using 500 sample points for all the kmeans models. Which is a better model and why ?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Davies Bouldin Index: \n",
      "\n",
      "The Davies Bouldin index attempts to combine both the above ideas and assigns a score so that we can identify models that produce clusters with low intra-cluster distances (high intra-cluster similarity) and high inter-cluster distances. To compute the Davies-Bouldin index, you will need to do the following:\n",
      "\n",
      "Consider the case where we have $N$ cluster centers. For each cluster center we first compute the average distance of all points in the cluster to the cluster center. Let us call these values $\\sigma_{i}$ and we will create values $\\sigma_{0}$ to $\\sigma_{N-1}$. \n",
      "\n",
      "Next for each cluster center we compute $d$, the distance between this center and every other cluster center. i.e If we have 5 cluster centers we will compute distances between every pair. \n",
      "\n",
      "Then, for each cluster center $i$ we find the maximum value of $\\frac{\\sigma_{i} + \\sigma_{j}}{d(c_{i}, c_{j})}$ where $j = 0$ to $N-1$ and $j \\neq i$. \n",
      "\n",
      "Finally we average the values from the previous step to obtain the Davies Bouldin index."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Give function skeletons for distance between two points (euclidean in 1000-dimension space)\n",
      "#\n",
      "\n",
      "# Provide skeleton for computing average distance from all points in a cluster\n",
      "\n",
      "# Skeleton here for computing distance between all cluster centers\n",
      "\n",
      "# Calculate the Davies Bouldin Index for all the kmeans models. Which is a better model and why ?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.4 External Evaluation\n",
      "To do this we are going to comapre the clusters \n",
      "obtained from K-Means with the set of tags that we got from user-labeled data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}