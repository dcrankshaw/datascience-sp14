{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CS194-16 Introduction to Data Science\n",
      "\n",
      "**Name**: *Please put your name*\n",
      "\n",
      "**Student ID**: *Please put your student ID*\n",
      "\n",
      "\n",
      "Assignment 2: Introduction to Machine Learning: Clustering and Classification\n",
      "===\n",
      "\n",
      "## Overview\n",
      "\n",
      "In this assignment, we will use machine learning techniques to perform data analysis and learn models about our data. We will use a real world music dataset from [Last.fm](http://last.fm) for this assignment. There are two parts to this assignment: In the first part we will look at Unsupervised Learning with clustering and in the second part, we will study Supervised Learning."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction to Machine Learning\n",
      "\n",
      "Machine learning is a branch of artifical intelligence where we try to find hidden structure within data. \n",
      "For example, lets say you are hired as a data scientist at a cool new music playing startup. You are given access to \n",
      "logs from the product and are asked find out what kinds of music are played on your website and how you can promote songs that will be \n",
      "popular. In this case we wish to extract some structure from the raw data we have using machine learning.\n",
      "\n",
      "There are two main kinds of machine learning algorithms:\n",
      "Unsupervised learning is the branch where we don't have any ground truth (or labeled data) that can help\n",
      "our training process. There are many approaches to unsupervised learning which includes topics like Clustering, \n",
      "Mixture Models, Hidden Markov Models etc. In this assignment we will predominantly look at clustering.\n",
      "In supervised learning we have training data which is labeled (either manually or from historical data) and we try to make\n",
      "predictions on new data.  **TODO: Same a few more words here**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Application\n",
      "\n",
      "Your assignment is to use machine learnings algorithms for two tasks on a real world music dataset from Last.fm. The goal in the first part is to cluster artists and try to discover all artists that belong a certain genre. **TODO: Say something about second part here**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Files\n",
      "\n",
      "Data files for this assignment can be found at:\n",
      "\n",
      "`https://github.com/amplab/datascience-sp14/raw/master/hw2/hw2data.tar.gz`\n",
      "\n",
      "The zip file includes the following files:\n",
      "\n",
      "* **artists-tags.csv**, User-defined tags for top artists\n",
      "* **userart-mat.csv**, Matrix mapping artist-id to users who have played songs by the artists\n",
      "\n",
      "We will explain the datasets and how they need to used in the assignment sections."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Deliverables\n",
      "\n",
      "Complete the all the exercises below and turn in a write up in the form of an IPython notebook, that is, **an .ipynb file**.\n",
      "The write up should include your code, answers to exercise questions, and plots of results.\n",
      "Complete submission instructions will be posted on Piazza.\n",
      "\n",
      "You can use this notebook and fill in answers inline, or if you prefer, do your write up in a separate notebook.\n",
      "In this notebook, we provide code templates for many of the exercises.\n",
      "They are intended to help with code re-use, since the exercises build on each other, and are highly recommended.\n",
      "Don't forget to include answers to questions that ask for natural language responses, i.e., in English, not code!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Guidelines\n",
      "\n",
      "#### Code\n",
      "\n",
      "This assignment can be done with basic python, matplotlib and scikit-learn.\n",
      "Feel free to use Pandas, too, which you may find well suited to several exercises.\n",
      "As for other libraries, please check with course staff whether they're allowed.\n",
      "\n",
      "You're not required to do your coding in IPython, so feel free to use your favorite editor or IDE.\n",
      "But when you're done, remember to put your code into a notebook for your write up.\n",
      "\n",
      "#### Collaboration\n",
      "\n",
      "This assignment is to be done individually.  Everyone should be getting a hands on experience in this course.  You are free to discuss course material with fellow students, and we encourage you to use Internet resources to aid your understanding, but the work you turn in, including all code and answers, must be your own work."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 0: Preliminaries\n",
      "\n",
      "### Exercise 0\n",
      "\n",
      "Download the data and unzip it. \n",
      "\n",
      "Read in the file **artists-tags.txt** and store the contents in a DataFrame or a list. The file format for this file is `artist-id|artist-name|tag|count`. The field mean the following:\n",
      "\n",
      "1. artist-id : a unique id for an artist (Formatted as a [MusicBrainz Identifier](https://musicbrainz.org/doc/MusicBrainz_Identifier))\n",
      "2. artist-name: name of the artist\n",
      "3. tag: user-defined tag for the artist\n",
      "4. count: number of times the tag was applied\n",
      "\n",
      "Similarly, read in the file **userart-mat.csv** . The file format for this file is `artist-id, user1, user2, .... user1000`. i.e. There are 1000 columns in this file and each column has a value 1 if the particular user played a song from this artist."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "DATA_PATH = \"/home/shivaram/projects/datascience-labs/hw2\" # Make this the /path/to/the/data\n",
      "\n",
      "def parse_artists_tags(filename):\n",
      "    df = pd.read_csv(filename, sep=\"|\", names=[\"ArtistID\", \"ArtistName\", \"Tag\", \"Count\"])\n",
      "    return df\n",
      "\n",
      "def parse_user_artists_matrix(filename):\n",
      "    df = pd.read_csv(filename)\n",
      "    return df\n",
      "\n",
      "artists_tags = parse_artists_tags(DATA_PATH + \"/artists-tags.txt\")\n",
      "user_art_mat = parse_user_artists_matrix(DATA_PATH + \"/userart-mat-training.csv\")\n",
      "\n",
      "print \"Number of tags %d\" % (artists_tags.Tag.count()) # Should be 952803\n",
      "print \"Number of artists %d\" % (user_art_mat.ArtistID.count()) # Should be 19021"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of tags 952803\n",
        "Number of artists 19021\n"
       ]
      }
     ],
     "prompt_number": 210
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 1: Finding genres by clustering\n",
      "\n",
      "The first task we will look at is how to discover artist genres by only looking at data from plays on Last.fm. One of the ways to do this is to use clustering. To evaluate how well our clustering algorithm performs we will use the user-generated tags and compare those to our clustering results. \n",
      "\n",
      "### 1.1 Data pre-processing\n",
      "\n",
      "Last.fm allows users to associate tags with every artist (See the [top tags](http://www.last.fm/charts/toptags) for a live example). However as there are a number of tags associated with every artists, in the first step we will pre-process the data and get the most popular tag for an artist."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 1\n",
      "\n",
      "**a**. For every artist in **artists_tags** calculate the most frequently used tag. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Implement this. You can change the function arguments if necessary\n",
      "# Return a data structure that you contains (artist id, artist name, top tag)\n",
      "def calculate_top_tag(all_tags):\n",
      "    grouped_sorted = all_tags.groupby('ArtistID').apply(lambda group: group.sort(['Count'], ascending=False).head(1))\n",
      "    return grouped_sorted.reset_index(drop=True)[['ArtistID', 'ArtistName', 'Tag']]    \n",
      "\n",
      "top_tags = calculate_top_tag(artists_tags)\n",
      "\n",
      "# Print the top tag for Nirvana\n",
      "# Artist ID for Nirvana is 003b2747-b74a-46c1-a51e-aeaffe88256c\n",
      "# Should be 'Grunge\n",
      "print \"Top tag for Nirvana is %s\" % (top_tags[top_tags.ArtistID == '5b11f4ce-a62d-471e-81fc-a69a8278c7da'].Tag.values[0]) # Complete this line "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top tag for Nirvana is Grunge\n"
       ]
      }
     ],
     "prompt_number": 211
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. To do clustering we will be using `numpy` matrices. Create a matrix from **user_art_mat** with every row in the matrix representing a single artist. The matrix will have 1000 columns, one for whether each user listened to the artist."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_user_matrix(input_data):\n",
      "    d = input_data.drop('ArtistID', axis=1)                        \n",
      "    return d.as_matrix()\n",
      "\n",
      "user_np_matrix = create_user_matrix(user_art_mat)\n",
      "\n",
      "print user_np_matrix.shape # Should be (19021, 846)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(19021, 846)\n"
       ]
      }
     ],
     "prompt_number": 212
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.2 K-Means clustering\n",
      "\n",
      "Having pre-processed the data we can now perform clustering on the dataset. In this assignment we will be using the python library \n",
      "[scikit-learn](http://scikit-learn.org/stable/index.html) for our machine learning algorithms. scikit-learn provides an extensive\n",
      "library of machine learning algorithms that can be used for analysis. Here is a [nice flow chart](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) that shows various algorithms implemented\n",
      "and when to use any of them. In this part of the assignment we will look at K-Means clustering\n",
      "\n",
      "> **Note on terminology**: \"samples\" and \"features\" are two words you will come across frequently when you look at machine learning papers or documentation. \"samples\" refer to data points that are used as inputs to the machine learning algorithm. For example in our dataset each artist is a \"sample\". \"features\" refers to some representation we have for every sample. For example the list of 1s and 0s we have for each artist are \"features\". Similarly the bag-of-words approach from the previous homework produced \"features\" for each document.\n",
      "\n",
      "#### K-Means algorithm\n",
      "\n",
      "Clustering is the process of automatically grouping data points that are similar to each other. In the [K-Means algorithm](http://en.wikipedia.org/wiki/K-means_clustering) we start with `K` initially chosen cluster centers (or centroids). We then compute the distance of every point from the centroids and assign each point to the centroid. Next we update the centroids by averaging all the points in the cluster. Finally, we repeat the algorithm until the cluster centers are stable.\n",
      "\n",
      "### Running K-Means\n",
      "\n",
      "#### K-Means interface\n",
      "Take a minute to look at the scikit-learn interface for calling [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). The constructor of the KMeans class returns a `estimator` on which you can call [fit](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit) to perform clustering.\n",
      "\n",
      "#### K-Means parameters\n",
      "From the above description we can see that there are a few parameters which control the K-Means algorithm. We will look at two parameters in the following exercises: \n",
      "\n",
      "1. Number of clusters: This needs to be chosen based on insights we have about the data. As we do not know how many genres exist we will try different values\n",
      "2. Initialization strategy: The basic initialization strategy for k-means is to pick random cluster centers initially. More recently a smart approach called 'kmeans++' has been proposed to improve convergence time. We wil compare both approaches.\n",
      "\n",
      "#### Timing your code\n",
      "We will also measure the performance of clustering algorithms in this section. You can time the code in a cell using the **%%time** [IPython magic](http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb) as the first line in the cell. \n",
      "\n",
      ">**Note**: By default, the scikit-learn KMeans implementation runs the algorithm 10 times with different center initializations. For this assignment you can run it just once by passing the `n_init` argument as 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 2\n",
      "\n",
      "**a**. Run K-means using *5* cluster centers on the `user_np_matrix` using *random* initialization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Run K-means using 5 cluster centers on user_np_matrix using the random initialization\n",
      "kmeans_5_random = KMeans(n_clusters=5, init='random', n_init=1)\n",
      "\n",
      "kmeans_5_random.fit(user_np_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 8.44 s, sys: 1.9 s, total: 10.3 s\n",
        "Wall time: 10.4 s\n"
       ]
      }
     ],
     "prompt_number": 213
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Run K-means using *5* cluster centers on the `user_np_matrix` using *kmeans++* initialization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "# Run K-means using 5 cluster centers on user_np_matrix using the kmeans++ initialization\n",
      "kmeans_5_smart = KMeans(n_clusters=5, init='k-means++', n_init=1)\n",
      "\n",
      "kmeans_5_smart.fit(user_np_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 11.6 s, sys: 2.84 s, total: 14.5 s\n",
        "Wall time: 14.5 s\n"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**. Run K-means using *25* cluster centers on the `user_np_matrix` using both *kmeans++* and random initialization. Also measure the time taken for both these cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "kmeans_25_random = KMeans(n_clusters=25, init='random', n_init=1)\n",
      "kmeans_25_random.fit(user_np_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 28.4 s, sys: 5.21 s, total: 33.6 s\n",
        "Wall time: 33.6 s\n"
       ]
      }
     ],
     "prompt_number": 215
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "kmeans_25_smart = KMeans(n_clusters=25, init='k-means++', n_init=1)\n",
      "kmeans_25_smart.fit(user_np_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 22.7 s, sys: 4.48 s, total: 27.2 s\n",
        "Wall time: 27.2 s\n"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "kmeans_100_smart = KMeans(n_clusters=100, init='k-means++', n_init=1)\n",
      "kmeans_100_smart.fit(user_np_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1min 10s, sys: 11.8 s, total: 1min 22s\n",
        "Wall time: 1min 22s\n"
       ]
      }
     ],
     "prompt_number": 217
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. Of the four algorithms, which setting took the longest to run ? Why do you think this is the case ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO Answer question"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.3 Evaluating K-Means\n",
      "\n",
      "In addition to the performance comparisons we also wish to compare how good our clusters are. To do this we are first going to look at internal evaluation metrics. For internal evaluation we only use the input data and the clusters created and try to measure the quality of clusters created. We are going to use three metrics for this:\n",
      "\n",
      "#### Inertia\n",
      "Inertia is a metric that is used to estimate how close the data points in a cluster are. This is calculated as the sum of squared distance for each point to it's closest centroid, i.e., its assigned cluster center. The intution behind inertia is that clusters with lower inertia are better as it means closely related points form a cluster.Inertia is calculated by scikit-learn by default.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 3**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**a**. Print inertia for all the kmeans model computed above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Inertia for KMeans with 5 clusters, random = %lf \" % (kmeans_5_random.inertia_)\n",
      "print \"Inertia for KMeans with 5 clusters, k-means++ = %lf\" % (kmeans_5_smart.inertia_)\n",
      "\n",
      "print \"Inertia for KMeans with 25 clusters, random =  %lf \" % (kmeans_25_random.inertia_)\n",
      "print \"Inertia for KMeans with 25 clusters, k-means++ = %lf \" % (kmeans_25_smart.inertia_)\n",
      "\n",
      "print \"Inertia for KMeans with 100 clusters, k-means++ = %lf \" % (kmeans_100_smart.inertia_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Inertia for KMeans with 5 clusters, random = 387023.056157 \n",
        "Inertia for KMeans with 5 clusters, k-means++ = 387026.606366\n",
        "Inertia for KMeans with 25 clusters, random =  354221.683229 \n",
        "Inertia for KMeans with 25 clusters, k-means++ = 355441.969864 \n",
        "Inertia for KMeans with 100 clusters, k-means++ = 327260.154769 \n"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Does KMeans run with 25 clusters have lower or greater inertia than the ones with 5 clusters ? Which algorithm is better and why ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">TODO: Answer question"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Silhouette Score: \n",
      "The silhouette score measures how close various clusters created are. A higher silhouette score is better as it means that we dont have too many overlapping clusters. The silhouette score can be computed using [sklearn.metrics.silhouette_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) from scikit learn."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c.** Calculate the Silhouette Score using 500 sample points for all the kmeans models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import silhouette_score\n",
      "\n",
      "def get_silhouette_score(data, model):\n",
      "    return silhouette_score(data, model.labels_, sample_size=1000)\n",
      "\n",
      "print \"Silhouette Score for KMeans with 5 clusters, random = %lf \" % get_silhouette_score(user_np_matrix, kmeans_5_random)\n",
      "print \"Silhouette Score for KMeans with 5 clusters, k-means++ = %lf\" % get_silhouette_score(user_np_matrix, kmeans_5_smart)\n",
      "\n",
      "print \"Silhouette Score for KMeans with 25 clusters, random =  %lf \" % get_silhouette_score(user_np_matrix, kmeans_25_random)\n",
      "print \"Silhouette Score for KMeans with 25 clusters, k-means++ = %lf \" % get_silhouette_score(user_np_matrix, kmeans_25_smart)\n",
      "\n",
      "print \"Silhouette Score for KMeans with 100 clusters, k-means++ = %lf \" % get_silhouette_score(user_np_matrix, kmeans_100_smart)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Silhouette Score for KMeans with 5 clusters, random = 0.246903 \n",
        "Silhouette Score for KMeans with 5 clusters, k-means++ = 0.274533"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Silhouette Score for KMeans with 25 clusters, random =  0.072382 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Silhouette Score for KMeans with 25 clusters, k-means++ = 0.099093 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Silhouette Score for KMeans with 100 clusters, k-means++ = -0.022021 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. How does increasing the number of clusters affect the silhouette score ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">TODO: Answer question"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Davies Bouldin Index: \n",
      "\n",
      "The Davies Bouldin index attempts to combine both the above ideas and assigns a score so that we can identify models that produce clusters with low intra-cluster distances (high intra-cluster similarity) and high inter-cluster distances. To compute the Davies-Bouldin index, you will need to do the following:\n",
      "\n",
      "Consider the case where we have $N$ cluster centers. For each cluster center we first compute the average distance of all points in the cluster to the cluster center. Let us call these values $\\sigma_{i}$ and we will create values $\\sigma_{0}$ to $\\sigma_{N-1}$. \n",
      "\n",
      "Next for each cluster center we compute $d$, the distance between this center and every other cluster center. i.e If we have 5 cluster centers we will compute distances between every pair. \n",
      "\n",
      "Then, for each cluster center $i$ we find the maximum value of $\\frac{\\sigma_{i} + \\sigma_{j}}{d(c_{i}, c_{j})}$ where $j = 0$ to $N-1$ and $j \\neq i$. \n",
      "\n",
      "Finally we average the values from the previous step to obtain the Davies Bouldin index.\n",
      "\n",
      "\n",
      ">**Distance between points**: For computing the DBI we need a function that will return the distance between any two points. We will use [Euclidean distance](http://en.wikipedia.org/wiki/Euclidean_distance) in this assignment. Scikit-learn has an [implementation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html#sklearn.metrics.pairwise.euclidean_distances) that you can use in your code."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**e.** Write a function to calculate the distance between all pairs of centroids. ($d(c_{i}, c_{j})$ from the text above) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import euclidean_distances\n",
      "\n",
      "def centroid_distances(kmeans_model):\n",
      "    centers = kmeans_model.cluster_centers_\n",
      "    shape = centers.shape\n",
      "    distances = {}\n",
      "    for i in xrange(0, shape[0]):\n",
      "        for j in xrange(0, shape[0]):\n",
      "            distances[(i, j)] = euclidean_distances(centers[i], centers[j])[0][0]\n",
      "    return distances\n",
      "\n",
      "c_distances = centroid_distances(kmeans_5_smart)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 220
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**f**. Write a function to calculate the average distance of all points in a cluster from  the cluster center. ($\\sigma_{i}$ from the text above)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Given a cluster number return the average distance of \n",
      "# all points in it from the centroid.\n",
      "def cluster_distance(kmeans_model, data, cluster_number):\n",
      "    center = kmeans_model.cluster_centers_[cluster_number]\n",
      "    labels = kmeans_model.labels_\n",
      "    sum_dist = 0.0\n",
      "    num_dist = 0.0\n",
      "    for i in xrange(0, data.shape[0]):\n",
      "        if labels[i] == cluster_number:\n",
      "            sum_dist += euclidean_distances(center, data[i,])[0][0]\n",
      "            num_dist += 1\n",
      "    return sum_dist / num_dist\n",
      "\n",
      "# Get average cluster distance for all clusters\n",
      "avg_cluster_distances = [cluster_distance(kmeans_5_smart, user_np_matrix, i) for i in xrange(0, 5)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 221
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**g**: Calculate the Davies Bouldin Index using the above functions given a KMeans model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "def calcuate_dbi(kmeans_model, data):\n",
      "    num_clusters = kmeans_model.n_clusters\n",
      "    avg_cluster_distances = \\\n",
      "        [cluster_distance(kmeans_model, data, i) for i in xrange(0, num_clusters)]\n",
      "    c_distances = centroid_distances(kmeans_model)\n",
      "    dbi_per_center = []\n",
      "    # For each cluster\n",
      "    for i in xrange(0, kmeans_model.n_clusters):\n",
      "        max_dbi = -float(\"inf\")\n",
      "        for j in xrange(0, kmeans_model.n_clusters):\n",
      "            if j != i:\n",
      "                db = (avg_cluster_distances[i] + avg_cluster_distances[j]) / c_distances[(i, j)]\n",
      "                max_dbi = max(db, max_dbi)\n",
      "        dbi_per_center.append(max_dbi)\n",
      "    return np.mean(dbi_per_center)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sweep_dbi(data):\n",
      "    num_centers = [2, 5, 25, 50, 100, 200, 500]\n",
      "    for c in num_centers:\n",
      "        kmeans_model = KMeans(n_clusters=c, n_init=1)\n",
      "        kmeans_model.fit(data)\n",
      "        dbi = calcuate_dbi(kmeans_model, data)\n",
      "        print \"DBI for %lf clusters is %lf\" % (c, dbi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 138
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**h**. Which is a better model according to DBI ? Why ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">TODO: Answer question"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.4 External Evaluation\n",
      "While internal evaluation is useful, a better method for measuring clustering quality is to do external evaluation. This might not be possible always as we may not have ground truth data available. In our application we will use the top tags from before as our ground truth data for external evaluation. For external evaluation we will predict tags for data from our **test** dataset.\n",
      "\n",
      "#### Exercise 3\n",
      "\n",
      "**a**. As a first step we will need to **join** the `artist_tags` data with the set of labels generated by K-Means model. That is, for every artist we will now have the top tag, cluster id and artist name in a data structure."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Return a data structure that contains artist_id, artist_name, tag, cluster_label for every artist\n",
      "def join_tags_labels(artists_data, user_data, kmeans_model):\n",
      "    artist_labels = pd.DataFrame({\"ArtistID\": user_data['ArtistID'], \"Label\":kmeans_model.labels_})\n",
      "    joined = pd.merge(artists_data, artist_labels, on='ArtistID')\n",
      "    return joined\n",
      "\n",
      "# Run the function for all the models\n",
      "kmeans_5_joined = join_tags_labels(top_tags, user_art_mat, kmeans_5_smart)\n",
      "kmeans_25_joined = join_tags_labels(top_tags, user_art_mat, kmeans_25_smart)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 223
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Next we need to generate a genre for every cluster id we have (the cluster ids are from 0 to N-1). You can do this by **grouping** the data from the previous exercise on cluster id. \n",
      "\n",
      "One thing you might notice is that we typically get a bunch of different tags associated with every cluster. How do we pick one genre or tag from this ? To cover various tags that are part of the cluster, we will pick the **top 5** tags in each cluster and save that as the list of top-5 tags as the genre for the cluster.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Return a data structure that contains cluster_id, list of top 5 tags for every cluster\n",
      "def assign_cluster_tags(joined_data):\n",
      "    grouped = kmeans_5_joined.groupby('Label')\n",
      "    pd.DataFrame(grouped.apply(lambda g: g.Tag.value_counts()[0:5].to_dict().keys())).reset_index()\n",
      "    \n",
      "kmeans_5_genres = assign_cluster_tags(kmeans_5_joined)\n",
      "kmeans_25_genres = assign_cluster_tags(kmeans_25_joined)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 224
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Evaluating Test Data\n",
      "Finally we evaluate the models using external test data. To do this we load the test data file **userart-mat-test.csv** and for every artist in the file we use the K-Means model to predict a cluster. We mark our prediction as successful if the artist's top tag belongs to one of the five tags for the cluster. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d** Load the testdata file and create a NumPy matrix named user_np_matrix_test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_art_mat_test = parse_user_artists_matrix(DATA_PATH + \"/userart-mat-test.csv\")\n",
      "user_np_matrix_test = create_user_matrix(user_art_mat_test)\n",
      "\n",
      "user_np_matrix_test.shape # Should be (6028, 846)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 226,
       "text": [
        "(19021, 846)"
       ]
      }
     ],
     "prompt_number": 226
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**e.** For each artist in the test set, call **[predict](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict)** to get the predicted cluster. Join the predicted labels with test artist ids. Return 'artist_id', 'predicted_label' for every artist in the test dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For every artist return a list of labels\n",
      "def predict_cluster(test_data, kmeans_model):\n",
      "    predicted_labels = kmeans_model.predict(test_data)\n",
      "    predicted = pd.DataFrame({\"ArtistID\": test_data['ArtistID'], \"Label\": predicted_labels})\n",
      "    return predicted\n",
      "\n",
      "kmeans_5_predicted = predict_cluster(user_np_matrix_test, kmeans_5_smart)\n",
      "kmeans_25_predicted = predict_cluster(user_np_matrix_test, kmeans_25_smart)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**f**. Get the tag for the predicted genre for the artist using the label and the tag for the artist from `top_tags`. Output for how many artists do we find the top tag in the predicted genre."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def verify_predictions(predicted_artist_labels, cluster_genres, top_tag_data):\n",
      "    predicted_dict = predicted_artist_labels.to_dict()\n",
      "    cluster_genres_dict = cluster_genres.to_dict()\n",
      "    \n",
      "    num_right = 0.0\n",
      "    for (artist, label) in predicted_dict: \n",
      "        artist_tag = top_tag_data[top_tag_data['ArtistID'] == artist].Tag\n",
      "        if artist_tag in cluster_genres_dict[label]:\n",
      "            num_right += 1.0\n",
      "    \n",
      "    return num_right\n",
      "\n",
      "kmeans_5_verified = verify_predictions(kmeans_5_predicted, kmeans_5_tags, top_tags)\n",
      "kmeans_25_verified = verify_predictions(kmeans_25_predicted, kmeans_25_tags, top_tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**g**. Print the accuracy for each KMeans model. We define accuracy as num_correct_predictions / num_artists_in_test_data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Accuracy of KMeans with 5 centers %lf \" % (kmeans_5_verified / user_np_matrix_test.shape[0])\n",
      "print \"Accuracy of KMeans with 25 centers %lf \" % (kmeans_25_verified / user_np_matrix_test.shape[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.5 Visualizing Clusters using PCA\n",
      "\n",
      "Another way to evaluate clustering is to visualize the output of clustering. However the data we are working with is in 846 dimensions !, so it is hard to visualize or plot this. Thus the first step for visualization is to reduce the dimensionality of the data. To do this we can use [Prinicipal Component Analysis (PCA)](http://en.wikipedia.org/wiki/Principal_component_analysis). PCA reduces the dimension of data and keeps only the most significant components of it. This is a commonly used technique to visualize data from high dimensional spaces.\n",
      "\n",
      "#### Exercise 4\n",
      "\n",
      "**a**. Calcluate the PCA of the training data set `user_np_matrix` and reduce it to 2 components. Use the [fit_transform](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform) method to do this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "\n",
      "def get_reduced_data(input_data):\n",
      "    reduced_data = PCA(n_components=2).fit_transform(input_data)\n",
      "    return reduced_data\n",
      "\n",
      "user_np_2d = get_reduced_data(user_np_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Fit the reduced data with the KMeans model with 5 cluster centers. Plot the cluster centers and all the points. Make sure to color points in every cluster differently to see a visual separation. You may find [`scatter`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter) and [`plot`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot) functions from matplotlib to be useful.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Write code to fit and plot data.\n",
      "import pylab as pl\n",
      "%pylab inline\n",
      "\n",
      "kmeans_5_reduced = KMeans(n_clusters=5, init='k-means++', n_init=2).fit(user_np_2d)\n",
      "color_code_map = {}\n",
      "color_code_map[0] = 'r'\n",
      "color_code_map[1] = 'b'\n",
      "color_code_map[2] = 'g'\n",
      "color_code_map[3] = 'm'\n",
      "color_code_map[4] = 'y'\n",
      "\n",
      "# Plot the centroids as a red X\n",
      "\n",
      "pl.figure(figsize=(10,5))\n",
      "for i in xrange(0, 5):\n",
      "    xs = user_np_2d[kmeans_5_reduced.labels_ == i, 0]\n",
      "    ys = user_np_2d[kmeans_5_reduced.labels_ == i, 1]\n",
      "    pl.plot(xs, ys, 'k.', markersize=3, color=color_code_map[i])\n",
      "\n",
      "centroids = kmeans_5_reduced.cluster_centers_\n",
      "pl.scatter(centroids[:, 0], centroids[:, 1],\n",
      "           marker='x', s=256, linewidths=3,\n",
      "           color='k')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}