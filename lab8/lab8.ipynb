{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Data Exploration Using Spark\n",
      "\n",
      "Today, we'll learn how to use [Spark](http://spark.incubator.apache.org/), a framework for large-scale data processing.  All of the frameworks that we've used so far (e.g., [Pandas](http://pandas.pydata.org/), R) are designed to be run on a single computer.  However, many data sets today are too large to be stored on a single computer.  Even when a dataset can be stored on one computer, the data set can often be processed much more quickly using multiple computers.  Spark is designed for this purpose: it allows you to concisely describe a program to analyze data on many computers, and hides many of the details of coordinating data analysis on many machines."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Getting Started\n",
      "\n",
      "We have a Spark cluster up and running for today's class.  The cluster includes worker machines, which will perform the data processing, and a master that keeps track of all of the workers.  The master runs a web UI that tells you about the cluster on port 8080.  Run the code below to print out the URL of the master's UI, and then copy that link into your browser to have a look at the UI."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We've setup the notebook so that the hostname of the master is saved\n",
      "# as CLUSTER_URL.\n",
      "master_ui_address = \"\".join(CLUSTER_URL.split(\"//\")[1].split(\":\")[0])\n",
      "print \"Master UI located at %s:8080\" % master_ui_address"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(response question: how many workers are running? how many cores and how much memory does each worker machine have?)\n",
      "\n",
      "You start a new Spark application by creating a SparkContext.  When the SparkContext is created, it asks the master for some executors.  An executor runs on one worker (but each worker may run multiple executors).  Your exectors are uniquely yours and won't be used for other peoples' jobs.  They store your application's data in memory, and will do work for your application.  Let's go ahead and start an application!\n",
      "\n",
      "(GRAPHIC HERE maybe?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext\n",
      "# Be sure to enter your own unique name here! If your name collides with someone else's, you'll get an\n",
      "# error that says \"ValueError: Cannot run multiple SparkContexts at once.\"\n",
      "# TODO(kay): is this true? or does this just happen if you try to start 2 in the same context?\n",
      "app_name = 'i<3datascience'\n",
      "print \"Starting SparkContext using master\", CLUSTER_URL\n",
      "sc = SparkContext(CLUSTER_URL, app_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Go back to the master web UI (refresh the page if you already have it open); now you should see your application listed under \"Running Applications.\"  Click on the ID of your application and you'll see a list of the executors that were assigned to your application.  Next, click \"Application Detail UI\" to get more details on our application.  The first page you'll see shows the jobs that you've run so far; there isn't any thing interesting here yet because we haven't run any jobs, but we'll return to this page later.\n",
      "\n",
      "(response question: how many total cores have been allocated for your application?)\n",
      "\n",
      "(thing to do in class: show how you can click on the executor to see that many other executors are running on the same machine)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Distributed Data\n",
      "\n",
      "One of the defining features of Spark compared to other data analytics frameworks (like Hadoop, which many of you used in CS61C) is that it stores data in memory rather than on disk.  This allows Spark applications to run much more quickly, because they aren't slowed by needing to read data from disk.  Let's load some data for our Spark application to use.\n",
      "\n",
      "GRAPHIC HERE: data stored in disk on HDFS. load it into memory on Spark.\n",
      "\n",
      "To load the data, we'll use `sc.textFile()`, which tells Spark create a new set of input data based on data read from a given input file path (in this case, `movielens/large/ratings.dat`).  In this case, the input file path points to a file in Hadoop Distributed File System (HDFS); HDFS stores the data on the disks of machines in the cluster.  Next, we call `cache()` on the new dataset to signal to Spark that this data should be kept in memory (for faster access in the future)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The first argument to textFile is a path to the data in HDFS.\n",
      "# The second argument specifies how many pieces to break the file\n",
      "# into; we'll talk more about this later in the tutorial.\n",
      "raw_ratings = sc.textFile(\"/movielens/large/ratings.dat\", 10)\n",
      "# Set the name so the RDD is easy to identify in the UI.\n",
      "# TODO: add this line  back when update to 0.9 branch\n",
      "# ratings.setName(\"ratings\")\n",
      "raw_ratings.cache()\n",
      "# Give our RDD a name so it's easily identifiable in the UI.\n",
      "raw_ratings.setName(\"raw ratings\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " Click on the \"Storage\" tab in your application UI.  You'll notice a few things here.  First, Spark calls datasets that it stores \"RDDs\" (resilient distributed datasets).  Second, the new dataset is not yet listed.  This is due to a feature of Spark called \"lazy evaluation\": Spark only computes a dataset or a transformation on a dataset when it's necessary to return a result.  Here, we haven't asked for any results from the `raw_ratings` dataset, so Spark avoids unnecessary work by not reading in the dataset yet.  To force Spark to read in the `raw_ratings` data, we'll count the entries in the dataset. `count()` requires the dataset to compute its result, so now Spark will read the data from HDFS."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entries = raw_ratings.count()\n",
      "print \"%s entries in ratings\" % entries"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reload the \"Storage\" tab in your application UI.  Now you should see the RDD you just created.  If you click on the RDD's name, you can see more information about where the RDD is stored: it's been split into ten different pieces (\"partitions\"), which are each stored on one of your executors.\n",
      "\n",
      "(response question: what's the total size of the ratings RDD? how big is each of the partitions?)\n",
      "\n",
      "One thing that's useful when we have a new dataset is to look at the first few entries to get a sense of what the data looks like.  In Spark, we do that using the `take()` command (analogous to the `head()` command in Pandas).  The format of each entry is `UserID::MovieID::Rating::Timestamp`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Look at the first 10 items in the dataset.\n",
      "raw_ratings.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Spark Operations\n",
      "\n",
      "So far, we've created a distributed dataset that's split into many partitions that are each stored on a single machine in our cluster.  Let's look at what happens when we do a basic operation on the dataset.  One of the most basic jobs that we can run is the `count()` job that we ran earlier.  Let's understand a little more about what happens when we run `count()` on the ratings dataset.  Go back to the UI for the application and click on the \"stages\" tab.  Under \"Completed Stages,\" click on your `count()` stage from earlier.  Scroll to the bottom and you'll see the tasks for the job.  A task is a unit of exeuction that runs on a single machine.  You can see the `count()` stage has the same number of tasks (10) as there are partitions of the ratings data set.  This is because each task runs on a single partition of the data set.  Each task counts the number of entries in its partition and sends that count to the driver, which adds up all of the counts.\n",
      "\n",
      "GRAPHIC: each map task counts the number of entries (show a smaller example, where we can see the # of things counted by each task)\n",
      "\n",
      "Count all of the entries in the ratings dataset again, and go back to the main \"Stages\" tab that summarizes all of the stages that have been run.  This page tells you some helpful information about each stage, including the number of tasks in the stage and how long it took.  How long did your new `count()` stage take?  How long did the old stage take?  Can you explain the difference?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_ratings.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let's convert the ratings dataset to a format that's a little easier to manipulate.  Having \"`::`\"-separated strings for each data item is not very convenient; let's convert the input data into tuples: `(UserID, MovieID, Rating, Timestamp)`, and convert the IDs, ratings, and timestamps to appropriate data types.  To do this, we'll use Spark's `map(f)` function.  When you call `map(f)` on a dataset, you pass in a function `f`.  Spark returns a new dataset formed by calling `f(x)` on each entry `x` in the original dataset.  Similar to with the `count()` operation, Spark launches one task for each partition of the distributed dataset.  Each task computes the `map()` on that partition and saves the result as a partition in the new dataset.\n",
      "\n",
      "(GRAPHIC: show a bunch of partitions, map working on each one)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tuple(entry):\n",
      "    items = entry.split(\"::\")\n",
      "    return int(items[0]), int(items[1]), float(items[2]), int(items[3]) \n",
      "\n",
      "ratings = raw_ratings.map(get_tuple)\n",
      "# Set the name of the new RDD, like we did before, so that it's easily\n",
      "# identifiable in the UI.\n",
      "ratings.setName(\"ratings\")\n",
      "ratings.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Click on the Storage tab in the UI again.  The new dataset is not listed because we haven't asked Spark to `cache()` this dataset.  Unless you explicitly ask Spark to save a dataset, it won't keep it in memory; instead, the `ratings` variable stores how to recompute `ratings` if you use it again.  We'll be using `ratings` a bunch more time so we want to keep it in memory for quick access, and drop the older `raw_ratings` dataset to clear up space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Cache ratings in memory and call count() to force Spark to bring it into memory.\n",
      "ratings.cache()\n",
      "ratings.count()\n",
      "\n",
      "# Remove raw_ratings from memory, since we don't need it anymore.\n",
      "raw_ratings.unpersist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`map()` was one kind of transformation; let's look at another transformation we can do using Spark.  Often, we want to filter data by removing data items that don't meet some criteria.  Spark provides `filter(f)` for this purpose.  `f` is a function that should take an entry and return `True` if the entry should be in the new dataset.; `filter(f)` outputs a new dataset composed of all of the entries of the original dataset for which `f` returns `True`.\n",
      "\n",
      "(GRAPHIC: what each task is doing)\n",
      "\n",
      "The movie ratings are between 0 and 5; let's count how many movies had a rating of at least 4."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, create a new RDD make up of the entries of new_ratings\n",
      "# that had a rating of at least 4.\n",
      "count = ratings.filter(lambda x: x[2] >= 4).count()\n",
      "print \"%s entries have ratings of at least 4\" % count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Time for you to do some work.  How many ratings did user 1 submit?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What fraction of movies have a rating of 5?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Parallelism\n",
      "\n",
      "When we created the ratings dataset, we asked Spark to split the dataset into 10 partitions, which we specified using the second parameter to `sc.textFile()`.  Let's look at what happens if we use a different number of partitions.  Read in the ratings dataset again using only 5 partitions, and call `count()` as we did the original dataset.  How long do you think `count()` will take now that the dataset is stored with half as many partitions?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE\n",
      "raw_ratings_fewer_partitions = # Read in the ratings data and split it into 5 partitions\n",
      "# Count the entries in the new dataset."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Can you explain the different in times between the count we did on this dataset and the count on the dataset with 10 partitions?  The number of partitions your data is split into and correspondingly, the number of tasks in each stage can have a big impact on how quickly your job runs.  You'll explore this more in homework 3."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multi-stage Jobs\n",
      "\n",
      "All of the jobs we've run so far have had just a single stage of tasks.  Let's look at a more complicated example where we do a map-reduce style of job, with two phases.  Let's count the number of entries with each rating value.  The diagram below shows how this will work.\n",
      "\n",
      "(GRAPHIC HERE)\n",
      "\n",
      "In the first stage of tasks, we map each entry to a 2-item tuple with the rating and the count of occurrences of the rating (the count is just 1 because each entry contains only a single rating).\n",
      "\n",
      "After the first stage, Spark re-organizes the data to group all of the tuples with the same rating by doing a \"shuffle\" over the network.  In this second, \"reduce\" stage, each task is responsible for a subset of the ratings, and collects the entries for those ratings from the appropriate map tasks.  Once each task has read all of the appropriate entries over the network, it adds up the counts for each rating."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_counts(x, y):\n",
      "    return x + y\n",
      "\n",
      "rating_counts = ratings.map(lambda x: (x[2], 1))\n",
      "aggregated_counts_rdd = rating_counts.reduceByKey(add_counts)\n",
      "print aggregated_counts_rdd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that Spark uses lazy evaluation: since we haven't asked Spark to do anything with the `aggregated_counts` RDD, it hasn't computed it yet.  Spark has just created a RDD object that stores how to compute `aggregated_counts` if we need it in the future.  In this case, the resulting `aggregated_counts` dataset will be small, so we want to just return the dataset as a Python list.  We can do this using the `collect()` function, which is like `take(x)` except that it returns the entire dataset rather than just the first `x` entries."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aggregated_counts_list = aggregated_counts_rdd.collect()\n",
      "print aggregated_counts_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use matplotlib to plot the results, similar to what we've done earlier in the class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plot\n",
      "# Magic command to make matplotlib and ipython play nicely together.\n",
      "%matplotlib inline\n",
      "width = 0.3\n",
      "ratings = [x[0] - width / 2 for x in aggregated_counts_list]\n",
      "counts = [x[1] for x in aggregated_counts_list]\n",
      "# The bar() function takes 2 lists: one list of x-coordinates of the left\n",
      "# side of each bar, and one list of bar heights.\n",
      "plot.bar(ratings, counts, width)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### DIY\n",
      "\n",
      "We've done a lot of work on the ratings dataset.  Now, it's time for you to do some processing a new dataset that stores information about each movie.  The movie dataset is stored at `/movielens/large/movies.dat` and each entry is formatted as `MovieID::Title::Genres`.  Read this dataset into memory and convert each entry to a tuple where the movie ID is an integer, the title is a String, and the genre is a list of strings.  How many total movies are there? (response form)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE\n",
      "def parse(entry):\n",
      "    items = entry.split(\"::\")\n",
      "    return (int(items[0]), items[1], items[2].split(\"|\"))\n",
      "\n",
      "movies = sc.textFile(\"/movielens/large/movies.dat\").map(parse).cache()\n",
      "print \"Number of movies: \", movies.count()\n",
      "movies.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How many movies are there for each genre?  You may find it useful to use Spark's `flatMap()` function, which is like `map()` except that the provided function maps each input item maps to a list containing 0 or more output items.  We've included a `plot_bars()` function for you that accepts a list of `(genre_name, count)` pairs and will make a bar graph."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_bars(genre_counts):\n",
      "    x_coords = range(len(genre_counts))\n",
      "    genre_names = [x[0] for x in genre_counts]\n",
      "    counts = [x[1] for x in genre_counts]\n",
      "    width = 0.8\n",
      "    plot.bar(x_coords, counts, width)\n",
      "    plot.xlabel(\"Genre\")\n",
      "    plot.ylabel(\"Number of Movies\")\n",
      "    plot.xticks([x + width/2.0 for x in x_coords], genre_names, rotation='vertical')\n",
      "\n",
      "### YOUR CODE HERE\n",
      "def get_genre_counts(x):\n",
      "    output = []\n",
      "    genres = x[2]\n",
      "    for g in genres:\n",
      "        output.append((g, 1))\n",
      "    return output\n",
      "\n",
      "genre_counts = movies.flatMap(get_genre_counts).reduceByKey(lambda x,y:x+y).collect()\n",
      "#reduceByKey(lambda x,y:x+y).collect()\n",
      "print genre_counts\n",
      "plot_bars(genre_counts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Joins\n",
      "\n",
      "Spark also supports joins. Joins operate on two data sets, where the entries in each dataset are (key, value) pairs.  `d1.join(d2)` returns all pairs `(k, (v1, v2))` such that `(k, v1)` was in `d1` and `(k, v2)` was in `d2`.  As an example of a join, let's compute the number of times that each movie was rated.  First, we'll compute the number of times that each movie was rated using the ratings dataset, and then we'll use a join to get the movie names from the movies dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# YOUR CODE HERE: Create a dataset of(rating, movieID) pairs.\n",
      "ratings_per_movie = ratings.map(lambda x: (x[1], 1)).reduceByKey(lambda x,y: x+y)# YOUR CODE HERE\n",
      "\n",
      "# YOUR CODE HERE: join average_ratings with movies to get a dataset with movie names and average ratings.\n",
      "ratings_with_names = movies.map(lambda x: (x[0], x[1])).join(ratings_per_movie)\n",
      "ratings_with_names.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which 10 movies have the highest average ratings?  You may want to use the `sortByKey()` function, which expects to be called on an RDD composed of (key, value) pairs and sorts them by key.  `sortByKey()` takes an optional parameter describing whether the keys should be sorted in ascending order."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE.\n",
      "sorted_ratings = ratings_with_names.map(lambda x: (x[1][1], (x[1][0], x[0]))).sortByKey(False)\n",
      "sorted_ratings.take(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Machine Learning\n",
      "\n",
      "We've learned about many of the basic transformations and actions that Spark allows us to do on distributed datasets.  Spark also exposes some higher level functionality; in particular, machine learning using a component of Spark called MLlib.  Today, we'll use MLlib to make personalized movie recommendations for you using the movie data we've been analyzing.\n",
      "\n",
      "Before jumping into the machine learning, we need to break up the dataset into a test set (which we'll use to train models), a validation set (which we'll use to choose the best model), and a test set.  One way that people often partition data is using the time stamp: using the 1's digit of the timestamp is an essentially random way to split the dataset into multiple groups.  Use the 1's digit of the rating timestamp to separate 60% of the data into a training set, 20% into a validation set, and the remaining 20% into a test set.  Call `cache()` on each dataset to store it in memory, since we'll be revisiting each dataset multiple times.\n",
      "\n",
      "(TODO: tell them about repartitioning here? do it earlier?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE\n",
      "# TODO: need to add your ratings to the training set.\n",
      "# TODO: add repartitioning -- need to update version of code first.\n",
      "num_partitions = 20\n",
      "training = ratings.filter(lambda x: x[3] % 10 < 6).cache() # YOUR CODE HERE \n",
      "validation = ratings.filter(lambda x: x[3] % 10 >= 6 and x[3] % 10 < 8).cache() # YOUR CODE HERE\n",
      "test = ratings.filter(lambda x: x[3] % 10 >= 8).cache() # YOUR CODE HERE\n",
      "\n",
      "print \"Training: %s, validation: %s, test: %s\" % (training.count(), validation.count(), test.count())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After splitting the dataset, your training set should have about 60K entries and the validation and test sets should have about 20K entries.\n",
      "\n",
      "Now, to generate movie recommendations, we're going to use a technique called collaborative filtering.  The basic idea of collaborative filtering is that we start with a matrix whose entries are movie ratings.  Each row represents a user and each column represents a particular movie.\n",
      "\n",
      "(MATRIX SHOWN HERE)\n",
      "\n",
      "We don't know all of the entries in this matrix, which is precisely why we need collaborative filtering.  For each user, we have ratings for only a subset of the movies.  With collaborative filtering, the idea is to approximate the ratings matrix as the product of two matrices: one that describes properties of each user, and one that describes properties of each movies.  (GRAPHIC).  We want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized (say more about ALS that kay doesn't know yet...like what the parameters mean)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.recommendation import ALS\n",
      "\n",
      "# Create test data that doesn't have the ratings yet.\n",
      "validation_without_ratings = validation.map(lambda x: (x[0], x[1])).cache()\n",
      "\n",
      "ranks = [8, 12]\n",
      "best_model = None\n",
      "lowest_error = float(\"inf\")\n",
      "best_rank = -1\n",
      "\n",
      "def compute_error(test_rdd, model):\n",
      "    test_data_without_ratings = test_rdd.map(lambda x: (x[0], x[1]))\n",
      "    # predictAll returns an RDD in the form (user, movie, rating).\n",
      "    predictions = model.predictAll(test_data_without_ratings)\n",
      "    # YOUR CODE HERE\n",
      "    # start the map thing here\n",
      "\n",
      "for rank in ranks:\n",
      "    model = ALS.train(training, rank)\n",
      "    # Output: RDD in the format (user, movie), rating\n",
      "    predictions = model.predictAll(validation_without_ratings).map(lambda r: ((r[0], r[1]), r[2]))\n",
      "    # YOUR CODE HERE: compute the root mean squared error\n",
      "    # Remember to put code that can be reused outside of this loop so it doesn't keep getting\n",
      "    # recomputed!\n",
      "    validation_count = validation_without_ratings.count()\n",
      "    ratings_and_preds = validation_without_ratings.map(lambda x: ((x[0], x[1]), x[2])).join(predictions)\n",
      "    # explain to people: can use reduce to just add everything together\n",
      "    total_error = ratings_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).reduce(lambda x,y:x+y)\n",
      "    error = total_error / validation_count\n",
      "    print \"Mean squared error =\", error\n",
      "    if error < lowest_error:\n",
      "        best_model = model\n",
      "        lowest_error = error\n",
      "        best_rank = rank\n",
      "        \n",
      "test_without_ratings = test.map(lambda x: x[0], x[1])\n",
      "test_predictions = model.predictAll(test_without_ratings).map(lambda r: ((r[0], r[1]), r[2]))\n",
      "\n",
      "print (\"The best model was trained with rank %s and it's root mean squared error on the test set is %s\" %\n",
      "       (best_rank, test_error))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# FAQ\n",
      "\n",
      "### I lost the UI! How do I get back there?\n",
      "\n",
      "The UI for the Spark master is on port 8080, and the hostname is stored as part of the `CLUSTER_URL` variable pre-loaded into this notebook.  From the master UI, you can find the link to your application's UI.\n",
      "\n",
      "### What are all of the operations I can do on a Spark dataset (RDD)?\n",
      "\n",
      "[This Page](http://spark.apache.org/docs/0.9.0/api/pyspark/index.html) lists all of the operations you can do on a Spark RDD.  Spark also has a Scala API (Scala is a programming language similar to Java); the [documentation for the Scala functions](http://spark.apache.org/docs/0.9.0/scala-programming-guide.html) is sometimes more helpful, and the Python functions work in the same way."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}