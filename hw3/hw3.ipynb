{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CS194-16: Introduction to Data Science\n",
      "\n",
      "__Name:__ *Please put your name*\n",
      "\n",
      "__Student ID:__ *Please put your student id*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Homework 3: Predicting Movie Ratings\n",
      "\n",
      "One of the most common uses of data is to predict what users want.  Predicting what users want allows websites to make more money by displaying more relevant ads, allows advertisers to make more money by targeting interested users, and allows online stores to make more money by recommending products to users.  While you may not like the idea of advertisers and stores figuring out what you want to make more money, it can also be useful to you as a consumer.  Sometimes those recommended products on Amazon or recommended movies on Netflix are actually things that you want!\n",
      "\n",
      "In this assignment, you'll explore how to recommend movies to a user.  We'll start with some basic methods, and then use machine learning to make more sophisticated predictions.\n",
      "\n",
      "We'll use Spark for this assignment.  As in the lab, we have a cluster up and running for you to use to do the homework.  Each group has been emailed the URL for a cluster that you can log into to complete the homework.  You should use the iPython notebook at that URL to complete your homework, this version is for reference only!  If you did not receive login information yet, please post a private note in Piazza ASAP so we can resolve the issue.\n",
      "\n",
      "We have created a [FAQ](#FAQ) at the bottom of this page to help with common problems you run into.  If you run into a problem, please check the FAQ before posting on Piazza!\n",
      "\n",
      "## Exercise 1: Basic Recommendations\n",
      "\n",
      "a) One way to recommend movies is to always recommend the movies with the highest average rating.  Based on the ratings and movies datasets we used in the lab (located at `movielens/large/ratings.dat` and `/movielens/large/movies.dat`, respectively), use Spark to find the name and average rating of the 10 movies with the highest average rating."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext\n",
      "### YOUR CODE HERE\n",
      "# As in the lab, the URL of the master is stored in CLUSTER_URL.\n",
      "# The web UI is running on the master at port 8080."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Which 10 movies have the highest average rating?__ *Your answer here*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "b) The movies you found may seem a bit suspicious.  How many ratings does each of those movies have?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "c) How can you improve your recommendations?  Improve upon your recommendations in part (a) to recommend 10 movies that you expect to be well-liked.  You are not expected to use any sophisticated machine learning techniques here; using just the Spark operations we learned in lab is sufficient."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Which 10 movies does your new code recommend?  Describe how you improved on the recommendations in part (a) in at most 4 sentences.__ *Your answer here*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise 2: Understanding Parallelism\n",
      "\n",
      "Before doing more sophisticated analysis of the movies dataset, let's optimize the performance.  In the lab, we saw that the number of partitions that a dataset is broken into can have a big affect on the response time of jobs run on that dataset.  In this part of the assignment, you'll determine the optimal number of partitions to break datasets into.\n",
      "\n",
      "Using Spark and [matplotlib](http://matplotlib.org/index.html), make a graph showing the response time of `count()` on the ratings dataset from the lab (located at `/movielens/large/ratings.dat`), using different numbers of tasks.  Remember that the number of tasks for `count()` is equal to the number of partitions in the dataset.  Make sure that you always call `count()` on a dataset that is in memory; otherwise the time to read the data from disk will skew your measurements."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plot\n",
      "# Magic command to make matplotlib and ipython play nicely together.\n",
      "%matplotlib inline\n",
      "\n",
      "### YOUR CODE HERE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Explain the shape of the plot in at most 3 sentences.__ *Your answer here*\n",
      "\n",
      "__What is the optimal number of partitions for this dataset and why?__ *Your answer here*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise 3 : Collaborative Filtering\n",
      "\n",
      "You've learned about many of the basic transformations and actions that Spark allows us to do on distributed datasets.  Spark also exposes some higher level functionality; in particular, machine learning using a component of Spark called MLlib.  In this assignment, you'll use MLlib to make personalized movie recommendations for you using the movie data we've been analyzing.\n",
      "\n",
      "We're going to use a technique called collaborative filtering.  The basic idea of collaborative filtering is that we start with a matrix whose entries are movie ratings.  Each row represents a user and each column represents a particular movie.\n",
      "\n",
      "(MATRIX SHOWN HERE)\n",
      "\n",
      "We don't know all of the entries in this matrix, which is precisely why we need collaborative filtering.  For each user, we have ratings for only a subset of the movies.  With collaborative filtering, the idea is to approximate the ratings matrix as the product of two matrices: one that describes properties of each user, and one that describes properties of each movies. (TODO: say more about this diagram)\n",
      "\n",
      "![foo](http://ampcamp.berkeley.edu/big-data-mini-course/img/matrix_factorization.png)\n",
      "\n",
      "We want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized (TODO: say more about ALS here).\n",
      "\n",
      "a) Before jumping into the machine learning, you need to break up the dataset into a test set (which we'll use to train models), a validation set (which we'll use to choose the best model), and a test set.  One way that people often partition data is using the time stamp: using the 1's digit of the timestamp is an essentially random way to split the dataset into multiple groups.  Use the 1's digit of the rating timestamp to separate 60% of the data into a training set, 20% into a validation set, and the remaining 20% into a test set.  After creating each dataset, use Spark's `repartition(numPartitions)` function (which re-partitions the dataset into the specified number of partitions) to break each dataset into the optimal number of partitions that you found in part 2."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE\n",
      "training = # YOUR CODE HERE \n",
      "validation = # YOUR CODE HERE\n",
      "test = # YOUR CODE HERE\n",
      "\n",
      "print \"Training: %s, validation: %s, test: %s\" % (training.count(), validation.count(), test.count())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After splitting the dataset, your training set should have about 60K entries and the validation and test sets should have about 20K entries (the exact number of entries in each dataset will vary slightly depending on the method you used to split the data into the 3 sets).\n",
      "\n",
      "b) In the next part, you'll generate a few different models, and will need a way to decide which model is best. We'll use the root mean squared error (RMSE) to compute the error of each model.  The root mean squared error is the square root of the average value of `(actual rating - predicted rating)^2` for all users and movies for which we have the actual rating."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__If your model perfectly predicts the user ratings, what will the root mean squared error be?__ *Your answer here*\n",
      "\n",
      "__If all of the predicted ratings are off by one (they're 1 higher or lower than the actual ratings), what will the RMSE be?__ *Your answer here*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write a function to compute the sum of squared error given a `predicted` and `actual` RDD."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_error(predicted, actual):\n",
      "    \"\"\" Compute the root mean squared error between predicted and actual.\n",
      "    \n",
      "    Params:\n",
      "      predicted: An RDD of predicted ratings for each movie and each user where each entry is in the form (user, movie, rating).\n",
      "      actual: An RDD of actual ratings where each entry is in the form (user, movie, rating).\n",
      "    \"\"\"\n",
      "    ### YOUR CODE HERE\n",
      "\n",
      "# sc.parallelize turns a Python list into a Spark RDD.\n",
      "test_predicted = sc.parallelize([\n",
      "    (1, 1, 5),\n",
      "    (1, 2, 3),\n",
      "    (1, 3, 4),\n",
      "    (2, 1, 3),\n",
      "    (2, 2, 2),\n",
      "    (2, 3, 4)])\n",
      "test_actual = sc.parallelize([\n",
      "     (1, 2, 3),\n",
      "     (1, 3, 5),\n",
      "     (2, 1, 5),\n",
      "     (2, 2, 1)])\n",
      "# The error for the test datasets should be 1.225.\n",
      "print \"Error for test datasets: %s\" % compute_error(test_predicted, test_actual)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "c) In this part, we'll use `ALS.train` to train a bunch of models, and we'll select the best model.  The most important parameter to ALS is the rank (TODO: explain what this is).  Train models with ranks of 4, 8, 12, and 16 using the `training` dataset, predict the ratings for the `validation` dataset, and use the `compute_error` function you wrote in part `(b)` to compute the error.  Which model has the lowest error? \n",
      "\n",
      "To create the model, use `ALS.train(training_rdd, rank)`, which takes two parameters: an RDD in the format (user, movie, rating) to use to train the model, and an integer rank.  To predict rating values, call `predictAll(test_RDD)` on the model generated with `ALS.train`.  `predictAll` accepts an RDD in the format (user, movie) and outputs an RDD in the format (user, movie, rating)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.recommendation import ALS\n",
      "\n",
      "best_rank = # YOUR CODE HERE\n",
      "\n",
      "print \"The best model was trained with rank %s\" % best_rank"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "d) So far, we used the `training` and `validation` datasets to select the best model.  Since we used these two datasets to determine what model is best, we can't use them to test how good the model is (otherwise we'd be vulnerable to overfitting).  To decide how good our model is, we need to use the `test` dataset.  Use the model you created in part (c) to predict the ratings for the test dataset and compute the RMSE."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_rmse = # YOUR CODE HERE\n",
      "\n",
      "print \"The model had a RMSE on the test set of %s\" % test_rmse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "e) As the final part of the assignment, predict what movies the model recommends for you.  First, create a unique user ID for yourself.  Find the highest user ID in the `ratings` dataset and use 1 + this ID as your user ID."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "my_user_id = # YOUR CODE HERE\n",
      "\n",
      "print \"My user ID is %s\" % my_user_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "f) Next, you need to create a RDD with some ratings for you.  In the lab, we outputted the most rated movies.  These are good candidates for you to use to add your own rankings.  Use the IDs and movies you've seen in that output to create an RDD with at least 10 rankings for yourself.  If you haven't seen at least 10 of these movies, you can use `take()` on the lab code to look at more of the most highly recommended movies.  Make sure to format your new RDD in the same way as the `training` RDD, so each entry is (userID, movieID, rating).  As we did in part (b) to test `compute_error`, you can use `sc.parallelize()` to make a Spark RDD from a Python list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def validate_ratings_RDD(ratings_rdd):\n",
      "    \"\"\" This function accepts a ratings RDD and makes sure that it's in the correct format.\"\"\"\n",
      "    # TODO(kay): write this\n",
      "    # make sure the user id isn't already in the ratings dataset\n",
      "    # make sure each rating is between 1 and 5\n",
      "    # make sure each movie ID is in the ratings dataset\n",
      "    return\n",
      "\n",
      "my_ratings_RDD = # YOUR CODE HERE\n",
      "\n",
      "validate_ratings_RDD(my_ratings_RDD)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "g) Spark's `union` method combines two RDDs (so `a.union(b)` creates a new RDD consisting of all of the entries in both `a` and `b`).  Use `union` to create a new training dataset that includes your ratings.  Train a new model using the best rank you found in part (c) (so you don't need to re-do the process of finding the best rank)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_model = # YOUR CODE HERE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "h)  Use the new model to predict your ratings on all of the movies that you didn't provide ratings for.  Print out the 10 movies with the highest predicted ratings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### YOUR CODE HERE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Wrapping Up\n",
      "\n",
      "Please answer these questions to help us improve the assignments for this class in the future.  If you thought all of the homeworks were equally difficult or equally enjoyable (or equally terrible!) please note that in your answers.  These answers are not part of your grade!\n",
      "\n",
      "__Which homework (1, 2, or 3) was the most difficult?__ *Your answer here*\n",
      "\n",
      "__Which homework (1, 2, or 3) was the least difficult?__ *Your answer here*\n",
      "\n",
      "__Which homework did you enjoy the most? Why?__ *Your answer here*\n",
      "\n",
      "__Which homework did you enjoy the least? Why?__ *Your answer here*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# FAQ <a name=\"FAQ\"></a>\n",
      "\n",
      "### How do I get to the UI?\n",
      "\n",
      "The UI for the Spark master is on port 8080, and the hostname is stored as part of the `CLUSTER_URL` variable pre-loaded into this notebook.  From the master UI, you can find the link to your application's UI.\n",
      "\n",
      "### What are all of the operations I can do on a Spark dataset (RDD)?\n",
      "\n",
      "[This Page](http://spark.apache.org/docs/0.9.0/api/pyspark/index.html) lists all of the operations you can do on a Spark RDD.  Spark also has a Scala API (Scala is a programming language similar to Java); the [documentation for the Scala functions](http://spark.apache.org/docs/0.9.0/scala-programming-guide.html) is sometimes more helpful, and the Python functions work in the same way.\n",
      "\n",
      "### How do I use matplotlib?\n",
      "\n",
      "There are lots of good examples on the [matplotlib website](http://matplotlib.org/index.html).  For example, [this page](http://matplotlib.org/examples/pylab_examples/simple_plot.html) shows how to plot a single line.\n",
      "\n",
      "### Python / Spark is giving me a crazy weird error!\n",
      "\n",
      "Spark is mostly written in Scala and Java, and the Python version of the code (\"pyspark\") hooks into the Java implementation in a way that can make error messages very difficult to understand.  If you get a hard-to-understand error when you run a Spark operation, we recommend first narrowing down the error so that you know exactly which operation caused the error.  For example, if `rdd.groupByKey().map(lambda x: x[1])` fails with an error, separate the `groupByKey()` and `map()` calls onto separate lines so you know which one is causing the error.  Next, double check the function signature to make sure you're passing the right arguments.  Pyspark can fail with a weird error if a RDD operation is given the wrong number or type of arguments.  If you're still stumped, try using `take(10)` to print out the first 10 entries in the dataset you're calling the RDD operation on.  Make sure the function you're calling and the arguments you're passing in make sense given the format of the input dataset."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}